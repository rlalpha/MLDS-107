{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from preposessing_data import generate_batch, data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_INPUT_SIZE = 4096\n",
    "HIDDEN_LAYER_SIZE = 1024\n",
    "EMBEDDING_SIZE = 1024\n",
    "BOS = 0\n",
    "EOS = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting load data......\n",
      "encoding data........\n",
      "adding special symbol......\n",
      "[[  0   4   5 ...   3   3   3]\n",
      " [  0   4  10 ...   3   3   3]\n",
      " [  0   4  15 ...   3   3   3]\n",
      " ...\n",
      " [  0   4   5 ...   3   3   3]\n",
      " [  0   8 397 ...   3   3   3]\n",
      " [  0   4  10 ...   3   3   3]] [[  4   5   6 ...   3   3   3]\n",
      " [  4  10  11 ...   3   3   3]\n",
      " [  4  15  16 ...   3   3   3]\n",
      " ...\n",
      " [  4   5 111 ...   3   3   3]\n",
      " [  8 397 507 ...   3   3   3]\n",
      " [  4  10  17 ...   3   3   3]]\n",
      "Done data generation!\n"
     ]
    }
   ],
   "source": [
    "X, y_inputs, y_targets, word_idx, idx_word, num_of_words, max_length, sequence_lengths = data_generator('./data/training_data', './data/training_label.json', 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "encoder_inputs = tf.placeholder(tf.float32, shape = [None, None, ENCODER_INPUT_SIZE])\n",
    "decoder_inputs = tf.placeholder(tf.int32, shape = [None, None])\n",
    "decoder_targets = tf.placeholder(tf.int32, shape = [None, None])\n",
    "sequence_length = tf.placeholder(tf.int32, shape = [None])\n",
    "sequence_length_fake = tf.placeholder(tf.int32, shape = [None])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_cell = tf.contrib.rnn.LSTMCell(HIDDEN_LAYER_SIZE)\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, \n",
    "                                       encoder_inputs, \n",
    "                                       dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define Decoder for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_cell = tf.contrib.rnn.LSTMCell(HIDDEN_LAYER_SIZE)\n",
    "output_layer = tf.layers.Dense(num_of_words, activation=tf.nn.softmax)\n",
    "# embedding for decoder\n",
    "embeddings = tf.Variable(tf.random_uniform([num_of_words, EMBEDDING_SIZE], -1.0, 1.0), dtype=tf.float32)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\n",
    "\n",
    "training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_inputs_embedded, \n",
    "                                           sequence_length_fake)\n",
    "training_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, \n",
    "                                          training_helper, \n",
    "                                          encoder_state,\n",
    "                                          output_layer)\n",
    "# unrolling the decoder layer\n",
    "training_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, \n",
    "                                                  impute_finished = True, maximum_iterations = max_length)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define Decoder for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings, \n",
    "                                                      tf.fill([BATCH_SIZE], BOS), \n",
    "                                                      EOS)\n",
    "\n",
    "inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, \n",
    "                                              inference_helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "\n",
    "inference_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, \n",
    "                                                      impute_finished=True, maximum_iterations = max_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define the training logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_logits = tf.identity(training_outputs.rnn_output, name='logits')\n",
    "masks = tf.sequence_mask(sequence_length, max_length, name = 'mask', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "training_logits\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Loss function - weighted softmax cross entropy\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        training_logits,\n",
    "        decoder_targets,\n",
    "        masks)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 23\n",
      "loss : 6.436155319213867\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 25\n",
      "loss : 6.393674373626709\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 25\n",
      "loss : 6.354521751403809\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 21\n",
      "loss : 6.321094989776611\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 24\n",
      "loss : 6.294537544250488\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 26\n",
      "loss : 6.243799686431885\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 30\n",
      "loss : 6.264980792999268\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 30\n",
      "loss : 6.270837306976318\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 26\n",
      "loss : 6.247490882873535\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 21\n",
      "loss : 6.245001792907715\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 40\n",
      "loss : 6.2640156745910645\n",
      "[ 7  8 18 ... 16  8 11]\n",
      "(32, 41) (32, 41) 29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f93799033ecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         _, loss= sess.run([train_op, cost], feed_dict= {encoder_inputs : X_batch, decoder_inputs : y_inputs_batch,\n\u001b[1;32m     14\u001b[0m                                               \u001b[0mdecoder_targets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_targets_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msequence_length_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                                                       sequence_length_fake : fake_max_sequence})\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss : {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training start\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epoc = 20\n",
    "fake_max_sequence = np.array([max_length] * BATCH_SIZE)\n",
    "for i in range(epoc):\n",
    "\n",
    "    for j in range(len(X) // BATCH_SIZE):\n",
    "        X_batch, y_inputs_batch, y_targets_batch, sequence_length_batch = generate_batch(X, y_inputs, y_targets, word_idx,\n",
    "         sequence_lengths, BATCH_SIZE)\n",
    "        print(y_inputs_batch.shape, y_targets_batch.shape, max(sequence_length_batch))\n",
    "        _, loss= sess.run([train_op, cost], feed_dict= {encoder_inputs : X_batch, decoder_inputs : y_inputs_batch,\n",
    "                                              decoder_targets: y_targets_batch, sequence_length : sequence_length_batch,\n",
    "                                                                      sequence_length_fake : fake_max_sequence})\n",
    "        print(\"loss : {0}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_max_sequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
